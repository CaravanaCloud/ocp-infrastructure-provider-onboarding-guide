{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome This site contains documentation about how to add a new infrastructure provider to the OpenShift platform. To begin learning about the process, please see the Overview .","title":"Welcome"},{"location":"#welcome","text":"This site contains documentation about how to add a new infrastructure provider to the OpenShift platform. To begin learning about the process, please see the Overview .","title":"Welcome"},{"location":"overview/","text":"Overview This document provides a high level view of the workflow for adding a new infrastructure provider to OpenShift. The following list is meant to give a general idea of the work involved with this process, the links contained within the list reference documents with greater detail for each step. Workflow Preparation Start with an idea to add a new infrastructure provider to OpenShift. Identify contacts within Red Hat, and the community, and then set expectations about the process ahead. Information Gathering Define the reference architecture and toplogy for OpenShift clusters on the new provider. Inventory existing components including licenses and plugin providers, and identify supported formats and infrastructure. OpenShift Enhancement Create an enhancment describing the new platform in detail, this will become a part of the official OpenShift enhancements . RHCOS Add support for the new platform to the Red Hat CoreOS Operating System. Installer New platforms should add support the OpenShift Installer, which users use to configure and build their OpenShift clusters. The Installer takes configuration and cloud credentials, validates the information, and builds the cloud infrastructure to create a cluster. Cloud Controller Manager New providers must have a Cloud Controller Manager integrated into OpenShift, including a repository in the OpenShift organization and container images included in the release payload. Container Storage Interface Driver Providers that expose storage options must have a CSI driver integrated into OpenShift, including a repository in the OpenShift organization and container images inclujded in the release payload. Machine API Controllers New providers must have a Machine actuator, and related controllers, for the Machine API Operator , including a repository in the OpenShift organization and container images included in the release payload. Network Ingress and DNS Evaluate provider-specific support for ingress load balancers and endpoint publishing strategies, as well as internal and external DNS support. Update and validate the Cluster Ingress Operator for the new infrastructure provider. Continuous Integration and Testing All components must have a suite of automation including unit style testing (run in isolation), and integration testing (run on cluster from the provider). This automation is part of the OpenShift development and release process, and exists within the OpenShift organization. Component Infrastructure Once OpenShift can be installed and operated on the new infrastructure platform attention should be given to the dynamic infrastructure services. These components are non-critical to the installation process and include things like the image registry. Documentation Product documentation is required for installation and maintenance tasks on the new infrastructure platform. Source level documentation is expected for all component repositories added to the OpenShift organization. Red Hat Relationship After the new infrastructure provider is ready for release, what are the commitments around maintenance and future releases. Release What to do once the code is approved and the release is ready for public consumption. How to promote and advertise, and work with Red Hat.","title":"Overview"},{"location":"overview/#overview","text":"This document provides a high level view of the workflow for adding a new infrastructure provider to OpenShift. The following list is meant to give a general idea of the work involved with this process, the links contained within the list reference documents with greater detail for each step.","title":"Overview"},{"location":"overview/#workflow","text":"Preparation Start with an idea to add a new infrastructure provider to OpenShift. Identify contacts within Red Hat, and the community, and then set expectations about the process ahead. Information Gathering Define the reference architecture and toplogy for OpenShift clusters on the new provider. Inventory existing components including licenses and plugin providers, and identify supported formats and infrastructure. OpenShift Enhancement Create an enhancment describing the new platform in detail, this will become a part of the official OpenShift enhancements . RHCOS Add support for the new platform to the Red Hat CoreOS Operating System. Installer New platforms should add support the OpenShift Installer, which users use to configure and build their OpenShift clusters. The Installer takes configuration and cloud credentials, validates the information, and builds the cloud infrastructure to create a cluster. Cloud Controller Manager New providers must have a Cloud Controller Manager integrated into OpenShift, including a repository in the OpenShift organization and container images included in the release payload. Container Storage Interface Driver Providers that expose storage options must have a CSI driver integrated into OpenShift, including a repository in the OpenShift organization and container images inclujded in the release payload. Machine API Controllers New providers must have a Machine actuator, and related controllers, for the Machine API Operator , including a repository in the OpenShift organization and container images included in the release payload. Network Ingress and DNS Evaluate provider-specific support for ingress load balancers and endpoint publishing strategies, as well as internal and external DNS support. Update and validate the Cluster Ingress Operator for the new infrastructure provider. Continuous Integration and Testing All components must have a suite of automation including unit style testing (run in isolation), and integration testing (run on cluster from the provider). This automation is part of the OpenShift development and release process, and exists within the OpenShift organization. Component Infrastructure Once OpenShift can be installed and operated on the new infrastructure platform attention should be given to the dynamic infrastructure services. These components are non-critical to the installation process and include things like the image registry. Documentation Product documentation is required for installation and maintenance tasks on the new infrastructure platform. Source level documentation is expected for all component repositories added to the OpenShift organization. Red Hat Relationship After the new infrastructure provider is ready for release, what are the commitments around maintenance and future releases. Release What to do once the code is approved and the release is ready for public consumption. How to promote and advertise, and work with Red Hat.","title":"Workflow"},{"location":"cloud-controller-manager/","text":"Cloud Controller Manager This document describes the necessary changes that must be made to add a new Cloud Controller Manager (CCM) to OpenShift. It does not cover the details of writing a CCM, for information about implementing the controller please see the official Kubernetes documentation on Developing Cloud Controller Manager as a starting point. Add the CCM repository to OpenShift One of the first things to do is copy the source code into the OpenShift organization on GitHub. This process is described in more detail in the Creating a GitHub Repository in the OpenShift Organization document. Having the source code in the OpenShift organization will allow Red Hat's continuous integration tooling access to it for running tests, building images, and including in the official releases. Configure basic repository integrations After setting up the code repository, there are several continuous integration tasks which must be done. This process is described in more detail in the Configuring Basic Repository Integrations document. As an example, here is the pull request to add the CCM for Alibaba Cloud to the OpenShift Prow. It configures the Tide mechanics as well as a pre-submit job to check the Go language formatting and a post-submit job to build the container images. openshift/release #19947 Integrating with other OpenShift components In OpenShift there are several operators which work together to ensure that the CCM is properly configured and also that new nodes which join the cluster are similarly configured for the infrastructure provider. To ensure that the new CCM and provider are properly recognized by all components there are several repositories which must be modified. openshift/api Before progressing with the OpenShift integrations, the new infrastructure provider will need to be present in the OpenShift API as part of the infrastructure.config.openshift.io definition. TODO: add link to installer doc step that describes updating openshift/api openshift/library-go In order for the new infrastructure provider to be recognized by the Cluster Cloud Controller Manager Operator, and other operators, the OpenShift library-go project must be updated so that the IsCloudProviderExternal function returns the proper response. As an example, here is the pull request to update the IsCloudProviderExternal function to add support for IBM Cloud. openshift/library-go #1161 It is important to note that any changes to library-go must be vendored into the projects which are dependent upon library-go. This process can often involve several repositories that must be updated before all the components will work together. To make tracking these changes easier, an issue can be used to monitor the individual changes. For example, this is an issue which tracked changes for the GCP and vSphere updates: openshift/cluster-cloud-controller-manager-operator #135 openshift/cluster-cloud-controller-manager-operator OpenShift uses the Cluster Cloud Controller Manager Operator (CCCMO) to manage the deployment and maintenance of the CCMs. This operator will deploy the individual containers of the CCM and ensure their health and continued operation during the cluster lifecycle. Adding a new infrastructure provider will require creating a package within the CCCMO that will contain the code for deploying the new components as well as the templated manifests for them. For a deeper discussion of integrating with the CCCMO, please see the developer documenation from that repository. As an example, here are two pull requests that added the IBM Cloud CCM to the CCCMO. The first pull request adds the primary code changes and manifests for deployment. The second pull request adds the CCM image references for the CCCMO to utilize when deploying from an OpenShift release payload. openshift/cluster-cloud-controller-manager-operator #97 openshift/cluster-cloud-controller-manager-operator #105","title":"Cloud Controller Manager"},{"location":"cloud-controller-manager/#cloud-controller-manager","text":"This document describes the necessary changes that must be made to add a new Cloud Controller Manager (CCM) to OpenShift. It does not cover the details of writing a CCM, for information about implementing the controller please see the official Kubernetes documentation on Developing Cloud Controller Manager as a starting point.","title":"Cloud Controller Manager"},{"location":"cloud-controller-manager/#add-the-ccm-repository-to-openshift","text":"One of the first things to do is copy the source code into the OpenShift organization on GitHub. This process is described in more detail in the Creating a GitHub Repository in the OpenShift Organization document. Having the source code in the OpenShift organization will allow Red Hat's continuous integration tooling access to it for running tests, building images, and including in the official releases.","title":"Add the CCM repository to OpenShift"},{"location":"cloud-controller-manager/#configure-basic-repository-integrations","text":"After setting up the code repository, there are several continuous integration tasks which must be done. This process is described in more detail in the Configuring Basic Repository Integrations document. As an example, here is the pull request to add the CCM for Alibaba Cloud to the OpenShift Prow. It configures the Tide mechanics as well as a pre-submit job to check the Go language formatting and a post-submit job to build the container images. openshift/release #19947","title":"Configure basic repository integrations"},{"location":"cloud-controller-manager/#integrating-with-other-openshift-components","text":"In OpenShift there are several operators which work together to ensure that the CCM is properly configured and also that new nodes which join the cluster are similarly configured for the infrastructure provider. To ensure that the new CCM and provider are properly recognized by all components there are several repositories which must be modified.","title":"Integrating with other OpenShift components"},{"location":"cloud-controller-manager/#openshiftapi","text":"Before progressing with the OpenShift integrations, the new infrastructure provider will need to be present in the OpenShift API as part of the infrastructure.config.openshift.io definition. TODO: add link to installer doc step that describes updating openshift/api","title":"openshift/api"},{"location":"cloud-controller-manager/#openshiftlibrary-go","text":"In order for the new infrastructure provider to be recognized by the Cluster Cloud Controller Manager Operator, and other operators, the OpenShift library-go project must be updated so that the IsCloudProviderExternal function returns the proper response. As an example, here is the pull request to update the IsCloudProviderExternal function to add support for IBM Cloud. openshift/library-go #1161 It is important to note that any changes to library-go must be vendored into the projects which are dependent upon library-go. This process can often involve several repositories that must be updated before all the components will work together. To make tracking these changes easier, an issue can be used to monitor the individual changes. For example, this is an issue which tracked changes for the GCP and vSphere updates: openshift/cluster-cloud-controller-manager-operator #135","title":"openshift/library-go"},{"location":"cloud-controller-manager/#openshiftcluster-cloud-controller-manager-operator","text":"OpenShift uses the Cluster Cloud Controller Manager Operator (CCCMO) to manage the deployment and maintenance of the CCMs. This operator will deploy the individual containers of the CCM and ensure their health and continued operation during the cluster lifecycle. Adding a new infrastructure provider will require creating a package within the CCCMO that will contain the code for deploying the new components as well as the templated manifests for them. For a deeper discussion of integrating with the CCCMO, please see the developer documenation from that repository. As an example, here are two pull requests that added the IBM Cloud CCM to the CCCMO. The first pull request adds the primary code changes and manifests for deployment. The second pull request adds the CCM image references for the CCCMO to utilize when deploying from an OpenShift release payload. openshift/cluster-cloud-controller-manager-operator #97 openshift/cluster-cloud-controller-manager-operator #105","title":"openshift/cluster-cloud-controller-manager-operator"},{"location":"component-infrastructure/","text":"Component Infrastructure TODO","title":"Component Infrastructure"},{"location":"component-infrastructure/#component-infrastructure","text":"TODO","title":"Component Infrastructure"},{"location":"container-storage-interface-driver/","text":"Container Storage Interface Driver TODO","title":"Container Storage Interface Driver"},{"location":"container-storage-interface-driver/#container-storage-interface-driver","text":"TODO","title":"Container Storage Interface Driver"},{"location":"continuous-integration-and-testing/","text":"Continuous Integration and Testing Continuous Integration (CI) testing for OpenShift and its supported infrastructure providers is handled through Prow; the same system used for the Kubernetes project. The landing page for OpenShift's CI can be found here , but we'll add some context for you as a newly supported OpenShift infrastructure provider. Some of the CI components will be hosted by you and some hosted by Red Hat. We'll start by looking at what you should have. Your Infrastructure You, as the infrastructure provider, will supply infrastructure where test OpenShift clusters will be installed and end-to-end (e2e) tests will be executed. The requirements vary from provider to provider. For example, VMWare and AWS are hosted very differently (on-premise vs hosted) so each one has caveats. But, there are some common requirements. API endpoint[s] for your service. i.e. What does your CLI or terraform communicate with to provision infrastructure? User accounts with required permissions for OpenShift installation. i.e. Can you share credentials that Red Hat can use to provision in your infrastructure? Ability to run multiple OpenShift clusters. i.e. Do you have enough resources to run multiple OpenShift clusters? More on this below. Network infrastructure including: DHCP Load balancing (Layer 4) Gateways and/or bastions for possible user access Sufficient storage for multiple OpenShift clusters. It is important to note that you will need to be able to run multiple OpenShift clusters at a time on your infrastructure. Why? Each code change related to your infrastructure provider will need to run unit and e2e tests. The e2e tests require an isolated OpenShift cluster so tests can run without interference. Its very likely to have more than one code change in flight at a time so having the ability to run multiple OpenShift clusters is necessary to quickly provide the test results to the engineers. In order to calculate your infrastructure size and costs, use this OpenShift Minimum Hardware Requirements document. Red Hat's Infrastructure Red Hat will host a cluster where Prow Jobs are run. These Prow Jobs will execute many steps which include infrastructure setup, provisioning a cluster on your infrastructure, running tests, and deprovisioning. The steps are a part of a chain and workflow (see here ). This Red Hat hosted cluster will need to be able to communicate with you so that it can run these steps, so some account and networking setup may be necessary. Connecting with OpenShift CI In each of the sections below, the outbound links contain examples of the code changes and pull requests. Create Cluster Profile Creating your Cluster Profile for OpenShift's CI is a major stepping stone to running CI jobs. Details on how to do this can be found here: https://docs.ci.openshift.org/docs/how-tos/adding-a-cluster-profile/. Red Hat will work with you to complete each of the steps since they will require review and code merges into the OpenShift CI system. Note that in the Providing Credentials step, you should work with Red Hat on how to access the Vault used for OpenShift CI. Adding to CI Step Registry OpenShift CI jobs are composed of multiple workflows and steps. You will need to add these to complete the integration with OpenShift CI. Documentation on creating workflows and steps can be found here . The registry of steps can be found here . Finally, an auto-generated document of all existing workflows, chains, and steps can be found here and is a great reference. Make sure that you have steps for each method of installation. For example, CI steps for UPI, IPI, disconnected network installations, or any other types.","title":"Continuous Integration"},{"location":"continuous-integration-and-testing/#continuous-integration-and-testing","text":"Continuous Integration (CI) testing for OpenShift and its supported infrastructure providers is handled through Prow; the same system used for the Kubernetes project. The landing page for OpenShift's CI can be found here , but we'll add some context for you as a newly supported OpenShift infrastructure provider. Some of the CI components will be hosted by you and some hosted by Red Hat. We'll start by looking at what you should have.","title":"Continuous Integration and Testing"},{"location":"continuous-integration-and-testing/#your-infrastructure","text":"You, as the infrastructure provider, will supply infrastructure where test OpenShift clusters will be installed and end-to-end (e2e) tests will be executed. The requirements vary from provider to provider. For example, VMWare and AWS are hosted very differently (on-premise vs hosted) so each one has caveats. But, there are some common requirements. API endpoint[s] for your service. i.e. What does your CLI or terraform communicate with to provision infrastructure? User accounts with required permissions for OpenShift installation. i.e. Can you share credentials that Red Hat can use to provision in your infrastructure? Ability to run multiple OpenShift clusters. i.e. Do you have enough resources to run multiple OpenShift clusters? More on this below. Network infrastructure including: DHCP Load balancing (Layer 4) Gateways and/or bastions for possible user access Sufficient storage for multiple OpenShift clusters. It is important to note that you will need to be able to run multiple OpenShift clusters at a time on your infrastructure. Why? Each code change related to your infrastructure provider will need to run unit and e2e tests. The e2e tests require an isolated OpenShift cluster so tests can run without interference. Its very likely to have more than one code change in flight at a time so having the ability to run multiple OpenShift clusters is necessary to quickly provide the test results to the engineers. In order to calculate your infrastructure size and costs, use this OpenShift Minimum Hardware Requirements document.","title":"Your Infrastructure"},{"location":"continuous-integration-and-testing/#red-hats-infrastructure","text":"Red Hat will host a cluster where Prow Jobs are run. These Prow Jobs will execute many steps which include infrastructure setup, provisioning a cluster on your infrastructure, running tests, and deprovisioning. The steps are a part of a chain and workflow (see here ). This Red Hat hosted cluster will need to be able to communicate with you so that it can run these steps, so some account and networking setup may be necessary.","title":"Red Hat's Infrastructure"},{"location":"continuous-integration-and-testing/#connecting-with-openshift-ci","text":"In each of the sections below, the outbound links contain examples of the code changes and pull requests.","title":"Connecting with OpenShift CI"},{"location":"continuous-integration-and-testing/#create-cluster-profile","text":"Creating your Cluster Profile for OpenShift's CI is a major stepping stone to running CI jobs. Details on how to do this can be found here: https://docs.ci.openshift.org/docs/how-tos/adding-a-cluster-profile/. Red Hat will work with you to complete each of the steps since they will require review and code merges into the OpenShift CI system. Note that in the Providing Credentials step, you should work with Red Hat on how to access the Vault used for OpenShift CI.","title":"Create Cluster Profile"},{"location":"continuous-integration-and-testing/#adding-to-ci-step-registry","text":"OpenShift CI jobs are composed of multiple workflows and steps. You will need to add these to complete the integration with OpenShift CI. Documentation on creating workflows and steps can be found here . The registry of steps can be found here . Finally, an auto-generated document of all existing workflows, chains, and steps can be found here and is a great reference. Make sure that you have steps for each method of installation. For example, CI steps for UPI, IPI, disconnected network installations, or any other types.","title":"Adding to CI Step Registry"},{"location":"documentation/","text":"Documentation All OpenShift components that are released from Red Hat will have accompanying documentation in the OpenShift product documentation . The content for these product documents are created and maintained by Red Hat, but they are inspired by the OpenShift enhancement , repository level documentation, and direct discussions with the component engineers. Repository level documentation Any component that has its code in a Git repository must have documentation in that repository to cover its usage and troubleshooting. This repository level documentation forms the basis of the product documentation and also helps OpenShift users to have a deeper understanding the software they are deploying and running. This documentation should be written by the authors of the software component. Examples of Respository Level Documentation Machine API Operator, the documenation directory contains several documents structured by intended audience and topic. Alibaba Machine API Actuator, the README file contains detailed information about deployment and usage. Product level documentation As mentioned above, all components in OpenShift will have product documentation created by Red Hat content teams. This documentation is inspired by the repository level content, and direct communication with the engineers who are creating and maintaining the software. During the development of new infrastructure components it is expected that Red Hat content writers will have access to the repository level documentation and to the original authors of the components in question.","title":"Documentation"},{"location":"documentation/#documentation","text":"All OpenShift components that are released from Red Hat will have accompanying documentation in the OpenShift product documentation . The content for these product documents are created and maintained by Red Hat, but they are inspired by the OpenShift enhancement , repository level documentation, and direct discussions with the component engineers.","title":"Documentation"},{"location":"documentation/#repository-level-documentation","text":"Any component that has its code in a Git repository must have documentation in that repository to cover its usage and troubleshooting. This repository level documentation forms the basis of the product documentation and also helps OpenShift users to have a deeper understanding the software they are deploying and running. This documentation should be written by the authors of the software component. Examples of Respository Level Documentation Machine API Operator, the documenation directory contains several documents structured by intended audience and topic. Alibaba Machine API Actuator, the README file contains detailed information about deployment and usage.","title":"Repository level documentation"},{"location":"documentation/#product-level-documentation","text":"As mentioned above, all components in OpenShift will have product documentation created by Red Hat content teams. This documentation is inspired by the repository level content, and direct communication with the engineers who are creating and maintaining the software. During the development of new infrastructure components it is expected that Red Hat content writers will have access to the repository level documentation and to the original authors of the components in question.","title":"Product level documentation"},{"location":"information-gathering/","text":"Information Gathering TODO","title":"Information Gathering"},{"location":"information-gathering/#information-gathering","text":"TODO","title":"Information Gathering"},{"location":"installer/","text":"Installer TODO","title":"Installer"},{"location":"installer/#installer","text":"TODO","title":"Installer"},{"location":"machine-api-controllers/","text":"Machine API Controllers The Machine API is OpenShift's declarative API for infrastructure provider compute resources. It is responsible for creating, destroying, and monitoring the machine instances (virtual machines, bare metal servers, etc.) that are configured for inclusion. In a standard Installer Provisioned Infrastructure (IPI) installation, the Machine API will include the instances of the compute plane, the control plane is not managed by the Machine API. In User Provisioned Infrastructure (UPI) installations, the Machine API may, or may not, be used depending on the configuration specified by the user. Components of Machine API There are several operators, controllers, and Custom Resource Definitions which define the Machine API. It is important to understand how these components interact before building a new infrastructure provider. Custom Resource Definitions The primary resources of the Machine API are Machines, MachineSets, and MachineHealthChecks. For the purposes of implementing a new infrastructure provider, Machines and MachineSets will be the resources that will require integration. Machines represent individual host instances within an infrastructure provider that will become Nodes in the OpenShift cluster. MachineSets are scalable collections of Machines. Within each MachineSet is a template for creating new Machines on the infrastructure. MachineHealthChecks are resources that are used to designate MachineSets which should automatically repair unhealthy Machines, and the conditions by which those Machine's health is determined. Machine API Operator The machine-api-operator (MAO) is the main entry point into the Machine API. The operator is responsible for deploying the infrastructure-specific Machine controller, and also runs controllers for MachineSets, and MachineHealthChecks. The MAO also deploys the validating and mutating webhooks for Machine resources. For more information about the Machine API Operator, please see the Machine API Operator Overview , and the Machine API Hacking Guide . Controllers owned by the Machine API Operator Machine controller - manages Machine resources. It uses an actuator interface , which follows a Machine lifecycle pattern . This interface provides Create , Update , Exists , and Delete methods to manage provider specific cloud instances, connected storage, and networking settings to make the instance prepared for bootstrapping. Each provider is therefore responsible for implementing these methods. MachineSet controller - manages MachineSet resources and ensures the presence of the expected number of replicas and a given provider config for a set of Machines. When increasing the replica count, this controller will use information in its ProviderSpec to create new Machines, copying that ProviderSpec to the Machine in the process. MachineHealthCheck controller - manages MachineHealthCheck resources. Ensure Machines being targeted by MachineHealthCheck objects are satisfying healthiness criteria or are remediated otherwise. For more information about MachineHealthChecks, please see the OpenShift Deploying machine health checks documentation. NodeLink controller - ensure Machines have a nodeRef based on providerID matching. Annotate nodes with a label containing the Machine name. Although the Machine API operator owns the lifecycle of all these controllers, it is worth noting that for new infrastructure providers only the Machine controller will require code changes. Machine API Webhooks The Machine API operator deploys mutating and validating webhooks for Machines and MachineSets. The webhooks allow the Machine API operator to detect invalid manifest declarations, or add values that are required by OpenShift. By using webhooks the Machine API operator is able to return warnings and errors to the user before the request is accepted by the Kubernetes API server. For more information about webhooks in Kubernetes, please see the Dynamic Admission Control documentation. In general, new infrastructure provider should not need to change the webhook configuration. Integrating with the Machine API Operator There are several steps involved with integrating a new infrastructure provider into the Machine API on OpenShift. The largest task will be creating the actuator which will do the work of creating, deleting, and updating infrastructure resources. Beyond that, there are some details around aligning the deployed release images, and the infrastrcuture detection within the MAO. Machine API CRDs and ProviderSpec MachineSets and Machines are the resources that describe the server instances within the infrastructure provider. Each MachineSet contains a Template field which declares how new Machines should be created by embedding a MachineSpec object. The MachineSpec contains a field named ProviderSpec , although this resource is not a CRD on its own, it is an embedded API type that allows each infrastructure provider to have a different set of variables to use when creating new Machines. While the MachineSet and Machine objects will not need to change to accomodate new infrastructure providers, the ProviderSpec will need to be created specifically for each new provider. For example here are the ProviderSpecs for AWS and IBM Cloud: ProviderSpec defined for AWS ProviderSpec defined for IBM Cloud In general, the ProviderSpec should be defined within the repository for the new infrastructure provider (akin to the IBM Cloud example). Although ProviderSpecs might be migrated into the openshift/api to consilidate API types in the future, it is expected that new providers will begin by developing their ProviderSpec within the code repository for their actuator. Starting with the ProviderSpec in the same repository as the actuator will promote faster initial development. Machine Controllers and Actuators As noted above, the Machine controller manages Machine resources. The Machine controller is also the interface between the MAO, the Machine API CRDs, and the infrastructure provider. The controller wraps the actuator interface which talks directly to the infrastructure provider. The actuator interface is the contact point between OpenShift's Machine API and a provider's infrastructure. This diagram illustrates the relationships: +----------+ +--------------------+ | Machine | reconciles | Machine controller | | Resource |<-------------| | +----------------+ +----------+ | +----------+ | communicates | | | | actuator |----+--------------->| Infrastructure | | +----------+ | | | +--------------------+ +----------------+ When writing a new actuator implementation, it is helpful to first start by looking at the controller wrapper which is provided by the MAO. The controller is defined in the controller.go file of the machine-api-operator repository. This controller is used by each infrastructure provider to associate its actuator code with the Machine reconciliation. The machine controller is designed to work with controller-runtime managers using the AddWithActuator method. For example, here is how the Machine controllers and actuators are configured for AWS and IBM Cloud: AWS Machine controller actuator configuration IBM Cloud Machine controller actuator configuration The majority of the work when implmenting a Machine controller for a new infrastructure provider will be satisfying the actuator interface defined by the MAO. The interface itself is small enough that it can be reproduced here: type Actuator interface { // Create the machine. Create(context.Context, *machinev1.Machine) error // Delete the machine. If no error is returned, it is assumed that all dependent resources have been cleaned up. Delete(context.Context, *machinev1.Machine) error // Update the machine to the provided definition. Update(context.Context, *machinev1.Machine) error // Checks if the machine currently exists. Exists(context.Context, *machinev1.Machine) (bool, error) } The following are two examples of Machine actuator implementations: AWS Machine actuator IBM Cloud Machine actuator MachineSet Controllers Infrastructure provider-specific MachineSet controllers are used to enable scale-from-zero features with the cluster autoscaler as described in this OpenShift enhancement on Autoscaling from/to zero . Unlike the Machine controller and actuator provided by the MAO, there is no corresponding helpers for implementing a MachineSet controller. The following are two examples of MachineSet controllers which implement scale from/to zero: AWS MachineSet controller IBM Cloud MachineSet controller Adding the new controllers to the Machine API operator With all the necessary controller code and repositories created, the final step for integration with the Machine API is adding the new infrastructure provider to the Machine API operator. This process involves adding a manifest for a cloud credential request, adding the container image references, and updating the operator to detect the new platform. These changes are best illustrated through example, the following pull request show the addition of the IBM Cloud provider to the Machine API operator. Take special note of the CredentialsRequest as this is the resource to encode provider-specific access roles. IBM Cloud provider addition to Machine API operator Integrating with OpenShift In addition to implementing the controllers and actuators for a new Machine API provider, the repository containing this code will need to be included in the OpenShift organization on GitHub. After adding the new repository to the organization, it will then need to be configured for continuous integration testing and image creation. Add Machine API provider repo to OpenShift For the highest levels of integration with OpenShift, it is recommended to copy the new infrastructure provider code repository to the OpenShift organization on GitHub. Migrating the code to this organization will help to ensure that the OpenShift continuous integration processes have full access to the source repository. To begin adding a new Machine API infrastructure provider repository the first step is the repository name. It should have a format of machine-api-provider-$PROVIDER_NAME , where $PROVIDER_NAME is replaced with the infrastructure name. For example, the AWS provider is machine-api-provider-aws . The repository will need to be copied into the OpenShift organization. This process is described in more detail in the Creating a GitHub Repository in the OpenShift Organization document. Having the source code in the OpenShift organization will allow Red Hat's continuous integration tooling access to it for running tests, building images, and including in the official releases. Configure basic repository integrations After setting up the code repository, there are several continuous integration tasks which must be done. This process is described in more detail in the Configuring Basic Repository Integrations document. As an example, here is the pull request to add the MAPI provider for IBM Cloud to the OpenShift Prow. It configures the Tide mechanics as well as pre-submit jobs to run the unit tests and check Go language formatting, and post-submit jobs to build the container images. openshift/release #19890 Relation to Cluster API The Machine API shares a common ancestry with the Cluster API project . In the early days of Cluster API development, the Machine API was solidified around the Machine and Machineset object definitions. Over time, the Machine API has continued to grow in accordance with the needs of OpenShift users. It is worth noting that the Machine API is not API compatible with the current versions of Cluster API.","title":"Machine API Controllers"},{"location":"machine-api-controllers/#machine-api-controllers","text":"The Machine API is OpenShift's declarative API for infrastructure provider compute resources. It is responsible for creating, destroying, and monitoring the machine instances (virtual machines, bare metal servers, etc.) that are configured for inclusion. In a standard Installer Provisioned Infrastructure (IPI) installation, the Machine API will include the instances of the compute plane, the control plane is not managed by the Machine API. In User Provisioned Infrastructure (UPI) installations, the Machine API may, or may not, be used depending on the configuration specified by the user.","title":"Machine API Controllers"},{"location":"machine-api-controllers/#components-of-machine-api","text":"There are several operators, controllers, and Custom Resource Definitions which define the Machine API. It is important to understand how these components interact before building a new infrastructure provider.","title":"Components of Machine API"},{"location":"machine-api-controllers/#custom-resource-definitions","text":"The primary resources of the Machine API are Machines, MachineSets, and MachineHealthChecks. For the purposes of implementing a new infrastructure provider, Machines and MachineSets will be the resources that will require integration. Machines represent individual host instances within an infrastructure provider that will become Nodes in the OpenShift cluster. MachineSets are scalable collections of Machines. Within each MachineSet is a template for creating new Machines on the infrastructure. MachineHealthChecks are resources that are used to designate MachineSets which should automatically repair unhealthy Machines, and the conditions by which those Machine's health is determined.","title":"Custom Resource Definitions"},{"location":"machine-api-controllers/#machine-api-operator","text":"The machine-api-operator (MAO) is the main entry point into the Machine API. The operator is responsible for deploying the infrastructure-specific Machine controller, and also runs controllers for MachineSets, and MachineHealthChecks. The MAO also deploys the validating and mutating webhooks for Machine resources. For more information about the Machine API Operator, please see the Machine API Operator Overview , and the Machine API Hacking Guide .","title":"Machine API Operator"},{"location":"machine-api-controllers/#controllers-owned-by-the-machine-api-operator","text":"Machine controller - manages Machine resources. It uses an actuator interface , which follows a Machine lifecycle pattern . This interface provides Create , Update , Exists , and Delete methods to manage provider specific cloud instances, connected storage, and networking settings to make the instance prepared for bootstrapping. Each provider is therefore responsible for implementing these methods. MachineSet controller - manages MachineSet resources and ensures the presence of the expected number of replicas and a given provider config for a set of Machines. When increasing the replica count, this controller will use information in its ProviderSpec to create new Machines, copying that ProviderSpec to the Machine in the process. MachineHealthCheck controller - manages MachineHealthCheck resources. Ensure Machines being targeted by MachineHealthCheck objects are satisfying healthiness criteria or are remediated otherwise. For more information about MachineHealthChecks, please see the OpenShift Deploying machine health checks documentation. NodeLink controller - ensure Machines have a nodeRef based on providerID matching. Annotate nodes with a label containing the Machine name. Although the Machine API operator owns the lifecycle of all these controllers, it is worth noting that for new infrastructure providers only the Machine controller will require code changes.","title":"Controllers owned by the Machine API Operator"},{"location":"machine-api-controllers/#machine-api-webhooks","text":"The Machine API operator deploys mutating and validating webhooks for Machines and MachineSets. The webhooks allow the Machine API operator to detect invalid manifest declarations, or add values that are required by OpenShift. By using webhooks the Machine API operator is able to return warnings and errors to the user before the request is accepted by the Kubernetes API server. For more information about webhooks in Kubernetes, please see the Dynamic Admission Control documentation. In general, new infrastructure provider should not need to change the webhook configuration.","title":"Machine API Webhooks"},{"location":"machine-api-controllers/#integrating-with-the-machine-api-operator","text":"There are several steps involved with integrating a new infrastructure provider into the Machine API on OpenShift. The largest task will be creating the actuator which will do the work of creating, deleting, and updating infrastructure resources. Beyond that, there are some details around aligning the deployed release images, and the infrastrcuture detection within the MAO.","title":"Integrating with the Machine API Operator"},{"location":"machine-api-controllers/#machine-api-crds-and-providerspec","text":"MachineSets and Machines are the resources that describe the server instances within the infrastructure provider. Each MachineSet contains a Template field which declares how new Machines should be created by embedding a MachineSpec object. The MachineSpec contains a field named ProviderSpec , although this resource is not a CRD on its own, it is an embedded API type that allows each infrastructure provider to have a different set of variables to use when creating new Machines. While the MachineSet and Machine objects will not need to change to accomodate new infrastructure providers, the ProviderSpec will need to be created specifically for each new provider. For example here are the ProviderSpecs for AWS and IBM Cloud: ProviderSpec defined for AWS ProviderSpec defined for IBM Cloud In general, the ProviderSpec should be defined within the repository for the new infrastructure provider (akin to the IBM Cloud example). Although ProviderSpecs might be migrated into the openshift/api to consilidate API types in the future, it is expected that new providers will begin by developing their ProviderSpec within the code repository for their actuator. Starting with the ProviderSpec in the same repository as the actuator will promote faster initial development.","title":"Machine API CRDs and ProviderSpec"},{"location":"machine-api-controllers/#machine-controllers-and-actuators","text":"As noted above, the Machine controller manages Machine resources. The Machine controller is also the interface between the MAO, the Machine API CRDs, and the infrastructure provider. The controller wraps the actuator interface which talks directly to the infrastructure provider. The actuator interface is the contact point between OpenShift's Machine API and a provider's infrastructure. This diagram illustrates the relationships: +----------+ +--------------------+ | Machine | reconciles | Machine controller | | Resource |<-------------| | +----------------+ +----------+ | +----------+ | communicates | | | | actuator |----+--------------->| Infrastructure | | +----------+ | | | +--------------------+ +----------------+ When writing a new actuator implementation, it is helpful to first start by looking at the controller wrapper which is provided by the MAO. The controller is defined in the controller.go file of the machine-api-operator repository. This controller is used by each infrastructure provider to associate its actuator code with the Machine reconciliation. The machine controller is designed to work with controller-runtime managers using the AddWithActuator method. For example, here is how the Machine controllers and actuators are configured for AWS and IBM Cloud: AWS Machine controller actuator configuration IBM Cloud Machine controller actuator configuration The majority of the work when implmenting a Machine controller for a new infrastructure provider will be satisfying the actuator interface defined by the MAO. The interface itself is small enough that it can be reproduced here: type Actuator interface { // Create the machine. Create(context.Context, *machinev1.Machine) error // Delete the machine. If no error is returned, it is assumed that all dependent resources have been cleaned up. Delete(context.Context, *machinev1.Machine) error // Update the machine to the provided definition. Update(context.Context, *machinev1.Machine) error // Checks if the machine currently exists. Exists(context.Context, *machinev1.Machine) (bool, error) } The following are two examples of Machine actuator implementations: AWS Machine actuator IBM Cloud Machine actuator","title":"Machine Controllers and Actuators"},{"location":"machine-api-controllers/#machineset-controllers","text":"Infrastructure provider-specific MachineSet controllers are used to enable scale-from-zero features with the cluster autoscaler as described in this OpenShift enhancement on Autoscaling from/to zero . Unlike the Machine controller and actuator provided by the MAO, there is no corresponding helpers for implementing a MachineSet controller. The following are two examples of MachineSet controllers which implement scale from/to zero: AWS MachineSet controller IBM Cloud MachineSet controller","title":"MachineSet Controllers"},{"location":"machine-api-controllers/#adding-the-new-controllers-to-the-machine-api-operator","text":"With all the necessary controller code and repositories created, the final step for integration with the Machine API is adding the new infrastructure provider to the Machine API operator. This process involves adding a manifest for a cloud credential request, adding the container image references, and updating the operator to detect the new platform. These changes are best illustrated through example, the following pull request show the addition of the IBM Cloud provider to the Machine API operator. Take special note of the CredentialsRequest as this is the resource to encode provider-specific access roles. IBM Cloud provider addition to Machine API operator","title":"Adding the new controllers to the Machine API operator"},{"location":"machine-api-controllers/#integrating-with-openshift","text":"In addition to implementing the controllers and actuators for a new Machine API provider, the repository containing this code will need to be included in the OpenShift organization on GitHub. After adding the new repository to the organization, it will then need to be configured for continuous integration testing and image creation.","title":"Integrating with OpenShift"},{"location":"machine-api-controllers/#add-machine-api-provider-repo-to-openshift","text":"For the highest levels of integration with OpenShift, it is recommended to copy the new infrastructure provider code repository to the OpenShift organization on GitHub. Migrating the code to this organization will help to ensure that the OpenShift continuous integration processes have full access to the source repository. To begin adding a new Machine API infrastructure provider repository the first step is the repository name. It should have a format of machine-api-provider-$PROVIDER_NAME , where $PROVIDER_NAME is replaced with the infrastructure name. For example, the AWS provider is machine-api-provider-aws . The repository will need to be copied into the OpenShift organization. This process is described in more detail in the Creating a GitHub Repository in the OpenShift Organization document. Having the source code in the OpenShift organization will allow Red Hat's continuous integration tooling access to it for running tests, building images, and including in the official releases.","title":"Add Machine API provider repo to OpenShift"},{"location":"machine-api-controllers/#configure-basic-repository-integrations","text":"After setting up the code repository, there are several continuous integration tasks which must be done. This process is described in more detail in the Configuring Basic Repository Integrations document. As an example, here is the pull request to add the MAPI provider for IBM Cloud to the OpenShift Prow. It configures the Tide mechanics as well as pre-submit jobs to run the unit tests and check Go language formatting, and post-submit jobs to build the container images. openshift/release #19890","title":"Configure basic repository integrations"},{"location":"machine-api-controllers/#relation-to-cluster-api","text":"The Machine API shares a common ancestry with the Cluster API project . In the early days of Cluster API development, the Machine API was solidified around the Machine and Machineset object definitions. Over time, the Machine API has continued to grow in accordance with the needs of OpenShift users. It is worth noting that the Machine API is not API compatible with the current versions of Cluster API.","title":"Relation to Cluster API"},{"location":"network-ingress-dns/","text":"Network Ingress and DNS Network Ingress Ensure the cloud provider implementation has support for load balancers If OpenShift should manage load balancers for this platform, the cloud provider needs to implement the cloudprovider.LoadBalancer interface . Then the ingress operator needs to be updated to configure a load balancer service on this platform. For example: - add provider-specific annotations to InternalLBAnnotations and managedLoadBalancerServiceAnnotations - evaluate customizations needed for load balancer service configuration in desiredLoadBalancerService - add unit tests to catch regressions Evaluate ingress provider-specific support for load balancers Review the end-to-end tests in the operator tests and make customizations, or skip the tests for this provider if they don't apply. For example: - see TestProxyProtocolOnAWS for an example for the AWS cloud provider - check if you require TestInternalLoadBalancer , TestIngressControllerCustomEndpoints , TestLocalWithFallbackOverrideForLoadBalancerService for your platform - add unit tests to catch regressions Evaluate provider-specific endpoint publishing strategy Look through the doc at the custom resource definition to understand the details for endpointPublishingStrategy , which is the set of parameters used to publish the ingress controller endpoints to other networks, enable load balancer integrations, and other tasks. Understand the properties: - hostNetwork, loadBalancerService, nodePortService, private Check if customizations are needed in the ingress controller operator For example: - add your default strategy to setDefaultPublishingStrategy - add your integration for IsProxyProtocolNeeded - add unit tests to catch regressions Document the default endpoint publishing strategy for the provider DNS Evaluate provider-specific DNS support and validate the controller Check if customizations are needed in the ingress operator's DNS controller . For example: - define a new DNS provider - add your platform type to createDNSProvider , and createDNSProviderIfNeeded - add unit tests to catch regressions Evaluate provider-specific externalDNS support (4.10+) Starting in OpenShift 4.10, there is the External-DNS operator to consider. It will support only these platforms in 4.10: - AWS - GCP - Azure Questions Questions can be directed to the OpenShift Slack channel #forum-network-edge","title":"Network Ingress and DNS"},{"location":"network-ingress-dns/#network-ingress-and-dns","text":"","title":"Network Ingress and DNS"},{"location":"network-ingress-dns/#network-ingress","text":"","title":"Network Ingress"},{"location":"network-ingress-dns/#ensure-the-cloud-provider-implementation-has-support-for-load-balancers","text":"If OpenShift should manage load balancers for this platform, the cloud provider needs to implement the cloudprovider.LoadBalancer interface . Then the ingress operator needs to be updated to configure a load balancer service on this platform. For example: - add provider-specific annotations to InternalLBAnnotations and managedLoadBalancerServiceAnnotations - evaluate customizations needed for load balancer service configuration in desiredLoadBalancerService - add unit tests to catch regressions","title":"Ensure the cloud provider implementation has support for load balancers"},{"location":"network-ingress-dns/#evaluate-ingress-provider-specific-support-for-load-balancers","text":"Review the end-to-end tests in the operator tests and make customizations, or skip the tests for this provider if they don't apply. For example: - see TestProxyProtocolOnAWS for an example for the AWS cloud provider - check if you require TestInternalLoadBalancer , TestIngressControllerCustomEndpoints , TestLocalWithFallbackOverrideForLoadBalancerService for your platform - add unit tests to catch regressions","title":"Evaluate ingress provider-specific support for load balancers"},{"location":"network-ingress-dns/#evaluate-provider-specific-endpoint-publishing-strategy","text":"Look through the doc at the custom resource definition to understand the details for endpointPublishingStrategy , which is the set of parameters used to publish the ingress controller endpoints to other networks, enable load balancer integrations, and other tasks. Understand the properties: - hostNetwork, loadBalancerService, nodePortService, private Check if customizations are needed in the ingress controller operator For example: - add your default strategy to setDefaultPublishingStrategy - add your integration for IsProxyProtocolNeeded - add unit tests to catch regressions","title":"Evaluate provider-specific endpoint publishing strategy"},{"location":"network-ingress-dns/#document-the-default-endpoint-publishing-strategy-for-the-provider","text":"","title":"Document the default endpoint publishing strategy for the provider"},{"location":"network-ingress-dns/#dns","text":"","title":"DNS"},{"location":"network-ingress-dns/#evaluate-provider-specific-dns-support-and-validate-the-controller","text":"Check if customizations are needed in the ingress operator's DNS controller . For example: - define a new DNS provider - add your platform type to createDNSProvider , and createDNSProviderIfNeeded - add unit tests to catch regressions","title":"Evaluate provider-specific DNS support and validate the controller"},{"location":"network-ingress-dns/#evaluate-provider-specific-externaldns-support-410","text":"Starting in OpenShift 4.10, there is the External-DNS operator to consider. It will support only these platforms in 4.10: - AWS - GCP - Azure","title":"Evaluate provider-specific externalDNS support (4.10+)"},{"location":"network-ingress-dns/#questions","text":"Questions can be directed to the OpenShift Slack channel #forum-network-edge","title":"Questions"},{"location":"openshift-enhancement/","text":"OpenShift Enhancement Inspired by the Kubernetes community enhancement process, OpenShift also uses enhancements to drive and define its features. Early in the process of adding a new infrastructure provider to OpenShift, an enhancement document should be created to scope the work that is being done and to expose design details which might need modification on OpenShift. To begin, visit the OpenShift enhancements repository and become familiar with the enhancement template as well as the enhancement guidelines . When ready, create a pull request with your new infrastructure provider enhancement to the repository. To aid in this process, here are two examples of pull requests for recently added platforms: Alibaba Cloud platform enhancement pull request IBM Cloud platform enhancement pull request Writing the enhancement The enhancement template should guide you in organizing the text of your enhancement. But, it has been designed primarily for feature and code changes to OpenShift, so there are a few topics you should consider specifically while writing. What will an OpenShift cluster look like on the new infrastructure provider? (Adding a topology diagram can be very helpful to explain the logical architecture.) Are there special networking considerations that need to be explained (e.g. DNS and load balancing strategies)? What types of compute and storage are available on the infrastructure? How will users install and consume OpenShift on the infrastructure? The \"Drawbacks\" and \"Alternatives\" sections in the enhancement do not always make sense for new infrastructure providers. Depending on the options available for deploying OpenShift on the infrastructure, the \"Alternatives\" section can be a good place to talk about other methods of deployment.","title":"OpenShift Enhancement"},{"location":"openshift-enhancement/#openshift-enhancement","text":"Inspired by the Kubernetes community enhancement process, OpenShift also uses enhancements to drive and define its features. Early in the process of adding a new infrastructure provider to OpenShift, an enhancement document should be created to scope the work that is being done and to expose design details which might need modification on OpenShift. To begin, visit the OpenShift enhancements repository and become familiar with the enhancement template as well as the enhancement guidelines . When ready, create a pull request with your new infrastructure provider enhancement to the repository. To aid in this process, here are two examples of pull requests for recently added platforms: Alibaba Cloud platform enhancement pull request IBM Cloud platform enhancement pull request","title":"OpenShift Enhancement"},{"location":"openshift-enhancement/#writing-the-enhancement","text":"The enhancement template should guide you in organizing the text of your enhancement. But, it has been designed primarily for feature and code changes to OpenShift, so there are a few topics you should consider specifically while writing. What will an OpenShift cluster look like on the new infrastructure provider? (Adding a topology diagram can be very helpful to explain the logical architecture.) Are there special networking considerations that need to be explained (e.g. DNS and load balancing strategies)? What types of compute and storage are available on the infrastructure? How will users install and consume OpenShift on the infrastructure? The \"Drawbacks\" and \"Alternatives\" sections in the enhancement do not always make sense for new infrastructure providers. Depending on the options available for deploying OpenShift on the infrastructure, the \"Alternatives\" section can be a good place to talk about other methods of deployment.","title":"Writing the enhancement"},{"location":"preparation/","text":"Preparation Welcome, this documentation describes the engineering related processes for adding a new infrastructure provider to the OpenShift Container Platform. If you are reading this, then you are most likely either planning to create a new provider or are in the process of implementing your provider for OpenShift. This documentation is intended to address the engineering-specific issues related to the process, if you have questions about other aspects of this process (e.g. legal, marketing, etc.) then you should reach out to your Red Hat representatives with those questions. Starting the process To begin, read through this documentation making note of the various components that will need to be configured for OpenShift. You will need to assess how your infrastructure meets the various requirements of OpenShift, and identify where gaps in feature coverage might exist. If you are creating an infrastructure provider that will be included with OpenShift Container Platform then you should already have a representative from Red Hat who will be your primary contact for questions and help with this process. If you have not contacted Red Het yet, please reach out through this Red Hat Partner Connect page for more information.","title":"Preparation"},{"location":"preparation/#preparation","text":"Welcome, this documentation describes the engineering related processes for adding a new infrastructure provider to the OpenShift Container Platform. If you are reading this, then you are most likely either planning to create a new provider or are in the process of implementing your provider for OpenShift. This documentation is intended to address the engineering-specific issues related to the process, if you have questions about other aspects of this process (e.g. legal, marketing, etc.) then you should reach out to your Red Hat representatives with those questions.","title":"Preparation"},{"location":"preparation/#starting-the-process","text":"To begin, read through this documentation making note of the various components that will need to be configured for OpenShift. You will need to assess how your infrastructure meets the various requirements of OpenShift, and identify where gaps in feature coverage might exist. If you are creating an infrastructure provider that will be included with OpenShift Container Platform then you should already have a representative from Red Hat who will be your primary contact for questions and help with this process. If you have not contacted Red Het yet, please reach out through this Red Hat Partner Connect page for more information.","title":"Starting the process"},{"location":"procedures/configuring-repository-integrations/","text":"Configuring Basic Repository Integrations After the repository is copied to the OpenShift organization, there are a few basic continuous integration tasks that should be configured. The most important of these initially are the job to permit automated merging through Tide and the job to create container images on commits. OpenShift uses Prow for continuous integration testing much like the Kubernetes community does. The configurations are contained in the openshift/release repository. This is where pull requests will need to be made to enable integrations for the new CCM. For a deeper understanding of the OpenShift continuous integration tooling, the OpenShift CI Docs are the authoritative source. In specific for this initial task, the Bootstrapping Configuration for a new Repository should be used as a guide.","title":"Configuring Basic Repository Integrations"},{"location":"procedures/configuring-repository-integrations/#configuring-basic-repository-integrations","text":"After the repository is copied to the OpenShift organization, there are a few basic continuous integration tasks that should be configured. The most important of these initially are the job to permit automated merging through Tide and the job to create container images on commits. OpenShift uses Prow for continuous integration testing much like the Kubernetes community does. The configurations are contained in the openshift/release repository. This is where pull requests will need to be made to enable integrations for the new CCM. For a deeper understanding of the OpenShift continuous integration tooling, the OpenShift CI Docs are the authoritative source. In specific for this initial task, the Bootstrapping Configuration for a new Repository should be used as a guide.","title":"Configuring Basic Repository Integrations"},{"location":"procedures/creating-an-openshift-repository/","text":"Creating a GitHub Repository in the OpenShift Organization A common task when adding a new component to OpenShift is creating a GitHub repository in the OpenShift organization . This is where all the code that becomes part of an OpenShift release lives. To accomplish creating a new repository, a Red Hat representative will need to make a Jira request on the internal DPP Board . For infrastructure implementors: Contact your Red Hat representative about creating a new repository. Have the name, description, and license information for the new repository ready to share. If you would like to have the repository forked from an existing repository let your contact know this at the beginning of the process. For Red Hat representatives: See this internal documentation, How to Request a new OpenShift GitHub Repository","title":"Creating a GitHub Repository in the OpenShift Organization"},{"location":"procedures/creating-an-openshift-repository/#creating-a-github-repository-in-the-openshift-organization","text":"A common task when adding a new component to OpenShift is creating a GitHub repository in the OpenShift organization . This is where all the code that becomes part of an OpenShift release lives. To accomplish creating a new repository, a Red Hat representative will need to make a Jira request on the internal DPP Board . For infrastructure implementors: Contact your Red Hat representative about creating a new repository. Have the name, description, and license information for the new repository ready to share. If you would like to have the repository forked from an existing repository let your contact know this at the beginning of the process. For Red Hat representatives: See this internal documentation, How to Request a new OpenShift GitHub Repository","title":"Creating a GitHub Repository in the OpenShift Organization"},{"location":"red-hat-relationship/","text":"Red Hat Relationship TODO","title":"Red Hat Relationship"},{"location":"red-hat-relationship/#red-hat-relationship","text":"TODO","title":"Red Hat Relationship"},{"location":"release/","text":"Release TODO","title":"Release"},{"location":"release/#release","text":"TODO","title":"Release"},{"location":"rhcos/","text":"RHCOS Platform Support To support a new platform one of the main requirements is to have Red Hat Enterprise Linux CoreOS (RHCOS) operating system work on the platform for provisioning the machines. This getting started guide gives a good overview of the Fedora CoreOS (FCOS) operating system and way to provision machines on different platforms. The process of building RHCOS and FCOS is largely the same except for the packages contained inside of the OS itself (RHEL vs Fedora). File a ticket for new platform support Please make sure to file a ticket against Fedora-CoreOS-Tracker by filing a new issue and choosing the Request a new platform option. Some important questions to be thought about/answered in the platform request tracker are: The official name of the platform - the name by which Ignition will identify the platform. Note that this can be different from the platform name used by the Openshift APIs How is userdata provided - this refers to how Ignition configuration is provided for this platform. How is the hostname provided - the method through which the machine gets its hostname. this could be through dhcp or a service like afterburn How is network configuration provided - the method through which network is configured - through DHCP or static ips provided in the metadata which can be accessed through services like afterburn How are ssh keys provided Details of the VM image which is to be built and published - this includes things like APIs to upload VM images, the disk format specification, etc.. Any new platform being added should first support FCOS (Fedora CoreOS) making sure that once the platform is fully supported that there are regular upstream CI jobs run to test FCOS and publish FCOS images. Support for building and publishing disk images If you are looking into building and publishing new disk images, you need to start with looking at the coreos-assembler tool. The coreos-assembler is a collection of tools inside a containerized build environment which is used to build and publish FCOS and RHCOS images. Some useful links on how to develop and use the coreos assembler: Working with CoreOS Assembler guide Building Fedora CoreOS Working on CoreOS Assembler guide CoreOS Assembler Design In addition, there are tools that help you test your image locally and also help you upload your images to the cloud. Mantle inside the coreos-assembler has a bunch of tools which help with different things: kola - for launching instances and running tests kolet - an agent for kola that runs on instances ore - for interfacing with cloud providers Ignition support Ignition is a utility that configures the machine. A list of supported platforms and how they provide the configuration to Ignition is listed here . Whenever a new platform is added, corresponding code must be added to specify the way to fetch configuration for the platform. Afterburn support Afterburn is used to implement cloud provider specific functionality by interacting with its metadata endpoints. It can be used to optionally set things like hostname and also inject network kernel command line arguments. In addition, it is also used to retrieve attributes from the metadata. Once added, some of this functionality can be enabled as systemd units in the Machine Config Operator. Questions Questions can be directed to these various communication channels for Fedora CoreOS","title":"RHCOS"},{"location":"rhcos/#rhcos-platform-support","text":"To support a new platform one of the main requirements is to have Red Hat Enterprise Linux CoreOS (RHCOS) operating system work on the platform for provisioning the machines. This getting started guide gives a good overview of the Fedora CoreOS (FCOS) operating system and way to provision machines on different platforms. The process of building RHCOS and FCOS is largely the same except for the packages contained inside of the OS itself (RHEL vs Fedora).","title":"RHCOS Platform Support"},{"location":"rhcos/#file-a-ticket-for-new-platform-support","text":"Please make sure to file a ticket against Fedora-CoreOS-Tracker by filing a new issue and choosing the Request a new platform option. Some important questions to be thought about/answered in the platform request tracker are: The official name of the platform - the name by which Ignition will identify the platform. Note that this can be different from the platform name used by the Openshift APIs How is userdata provided - this refers to how Ignition configuration is provided for this platform. How is the hostname provided - the method through which the machine gets its hostname. this could be through dhcp or a service like afterburn How is network configuration provided - the method through which network is configured - through DHCP or static ips provided in the metadata which can be accessed through services like afterburn How are ssh keys provided Details of the VM image which is to be built and published - this includes things like APIs to upload VM images, the disk format specification, etc.. Any new platform being added should first support FCOS (Fedora CoreOS) making sure that once the platform is fully supported that there are regular upstream CI jobs run to test FCOS and publish FCOS images.","title":"File a ticket for new platform support"},{"location":"rhcos/#support-for-building-and-publishing-disk-images","text":"If you are looking into building and publishing new disk images, you need to start with looking at the coreos-assembler tool. The coreos-assembler is a collection of tools inside a containerized build environment which is used to build and publish FCOS and RHCOS images. Some useful links on how to develop and use the coreos assembler: Working with CoreOS Assembler guide Building Fedora CoreOS Working on CoreOS Assembler guide CoreOS Assembler Design In addition, there are tools that help you test your image locally and also help you upload your images to the cloud. Mantle inside the coreos-assembler has a bunch of tools which help with different things: kola - for launching instances and running tests kolet - an agent for kola that runs on instances ore - for interfacing with cloud providers","title":"Support for building and publishing disk images"},{"location":"rhcos/#ignition-support","text":"Ignition is a utility that configures the machine. A list of supported platforms and how they provide the configuration to Ignition is listed here . Whenever a new platform is added, corresponding code must be added to specify the way to fetch configuration for the platform.","title":"Ignition support"},{"location":"rhcos/#afterburn-support","text":"Afterburn is used to implement cloud provider specific functionality by interacting with its metadata endpoints. It can be used to optionally set things like hostname and also inject network kernel command line arguments. In addition, it is also used to retrieve attributes from the metadata. Once added, some of this functionality can be enabled as systemd units in the Machine Config Operator.","title":"Afterburn support"},{"location":"rhcos/#questions","text":"Questions can be directed to these various communication channels for Fedora CoreOS","title":"Questions"}]}